Chapter 4: The Ultimate Dynamic Programming Algorithm

Recall that in chapter one, we managed to find a mapping from a 52-bit restricted binary to a hash key ranged from 0 to 133784559. Although the number of entries in this hash table is considerably large, it's still feasible on modern computers. If we could manage to improve the time efficiency in the hash function, it's still a useful approach.

Let's go back to that problem HashNBinaryKSum:

 Problem: HashNBinaryKSum

 Input: integer n, integer k, an n-bit binary with exactly k bits set to 1

 Output: the position of the binary in the lexicographical ordering of all n-bit
 binaries with exactly k bits of ones

More specificly, we are trying to solve a problem with n = 52 and k = 7. If we split the 52-bit binary into 4 blocks, where each block has 13 bits, we can precompute the results in a table in size 2^13 * 4 * 8, and do only 4 summations in the actual hash function. In practice, it would be easier if we use a 16-bit block instead of 13, making the table in size 2^16 * 4 * 8.

Precomputing this table is similar to the methods we used in the previous chapters. I'll just put the sample C code here and omit the explanations.

{
	int dp[65536][4][8];
	for (i=0; i<65536; i++)
	{
		for (j=0; j<4; j++)
		{
			for (k=0; k<8; k++)
			{
				int ck = k;
				int s;
				int sum = 0;

				for (s=15; s>=0; s--)
				{
					if (i & (1 << s))
					{
						int n = j*16 + s;
						sum += choose[n][ck];
						ck--;
					}
				}
				dp[i][j][k] = sum;
			}
		}
	}
}

And the hash function only need to sum up the result from the dp table. The C code is shown below.

int fast_hash(unsigned long long handid, int k)
{
	int hash = 0;
	unsigned short * a = (unsigned short *)&handid;

	hash += dp_fast[a[3]][3][k];
	k -= bitcount[a[3]];

	hash += dp_fast[a[2]][2][k];
	k -= bitcount[a[2]];

	hash += dp_fast[a[1]][1][k];
	k -= bitcount[a[1]];

	hash += dp_fast[a[0]][0][k];

	return hash;
}

Although this algorithm takes very few CPU cycles to compute the hash value (4 summations and 3 decrements), the overall performance is worse than what we used in the previous chapter. Part of the reason might be the dp table is greater than a normal page size (64Kbytes). If we cut the block into 8 bits and use a table of size 2^8 * 8 * 8, which will double the  number of operations in the hash function (8 summations and 7 decrements), the performance seems to improve, but still doesn't beat the algorithm used in the previous chapter under my environment.

In summary, although it's an algorithm that uses very few CPU cycles, the true performance is bounded by the time access to the memory, which doesn't make it faster than the algorithm we used in chapter 2 to chapter 3.
